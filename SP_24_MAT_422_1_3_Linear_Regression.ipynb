{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnvPnUsIrETMsPIHo3zSUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JadonTelep/MAT-422/blob/main/SP_24_MAT_422_1_3_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 1 - Linear Algebra**"
      ],
      "metadata": {
        "id": "WhamUuQ6Z_n1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Linear Regression"
      ],
      "metadata": {
        "id": "-U7XXRmOaBn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression is used frequently in practical applications because of\n",
        "its simplicity. The models depend linearly on their unknown parameters\n",
        "and, therefore, are easier to fit than models which are non-linearly related\n",
        "to their parameters. As a result, the statistical properties of the resulting\n",
        "estimators are easier to determine. In this section, we first discuss QR decomposition, the least-squares problem, and return to linear regression."
      ],
      "metadata": {
        "id": "4A9r4FhVaK3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.1 QR Decomposition"
      ],
      "metadata": {
        "id": "AH365M67aLLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QR decomposition is a useful procedure to solve the linear least squares problem. First, we use the Gram-Schmidt algorithm to obtain an orthonormal basis $\\operatorname{span}\\left(\\mathbf{a}_{1}, \\ldots, \\mathbf{a}_{m}\\right)$ from a linearly independent set of $\\operatorname{span}\\left(\\mathbf{a}_{1}, \\ldots, \\mathbf{a}_{m}\\right)$. In order to derive QR decomposition, let\n",
        "$$\n",
        "A=\\left(\\begin{array}{ccc}\n",
        "\\mid & & \\mid \\\\\n",
        "\\mathbf{a}_{1} & \\ldots & \\mathbf{a}_{m} \\\\\n",
        "\\mid & & \\mid\n",
        "\\end{array}\\right) \\quad \\text { and } \\quad \\mathrm{Q}=\\left(\\begin{array}{ccc}\n",
        "\\mid & & \\mid \\\\\n",
        "\\mathbf{q}_{1} & \\ldots & \\mathbf{q}_{m} \\\\\n",
        "\\mid & & \\mid\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "where $A$, $Q$ are $n \\times m$ matrices. The output of the Gram-Schmidt algorithm above can then be written in the following compact form, known as a QR decomposition in Fig. 1.3,\n",
        "$$\n",
        "A=\\mathrm{QR},\n",
        "$$\n",
        "where column $i$ of the $m \\times m$ matrix $R$ contains the coefficients of the linear combination of $\\mathbf{q}_{j}$ 's that produces $\\mathbf{a}_{i} . \\mathrm{Q}$ is a $\\mathbb{R}^{n \\times m}$ matrix with $Q^{T} \\mathrm{Q}=I_{m \\times m}$. It may be easier to verify $A=\\mathrm{QR}$ by\n",
        "$$\n",
        "A^{T}=R^{T} Q^{T} .\n",
        "$$\n",
        "\n",
        "By the proof of Gram-Schmidt, $\\mathbf{a}_{i} \\in \\operatorname{span}\\left(\\mathbf{q}_{1}, \\ldots, \\mathbf{q}_{i}\\right)$. So column $i$ of $R$ has only zeros below the diagonal. Hence $R$ has a special structure; it is upper triangular."
      ],
      "metadata": {
        "id": "UEzg2DhsaNDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Untitled.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "2ouBHEG_bQsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.2 Least-Squares Problems"
      ],
      "metadata": {
        "id": "PoRs7c4tbjV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $A \\in \\mathbb{R}^{n \\times m}$ be an $n \\times m$ matrix and $\\mathbf{b} \\in \\mathbb{R}^{n}$ be a vector. We try to solve the system $A \\mathbf{x}=\\mathbf{b}$, which is often inconsistent. We are looking to use the $A \\mathbf{x}$ to approximate $\\mathbf{b}$. It is reasonable to assume that matrix $A$ has linearly independent columns. If $n=m$, that is, if $A$ is a square matrix, we can use the matrix inverse to solve the system. But we are particularly interested in the over-determined case where $n>m$. We cannot use the matrix inverse then. One possibility to make sense of the problem in that case is to cast it as the least-squares problem:\n",
        "$$\n",
        "\\min _{\\mathbf{x} \\in \\mathbb{R}^{m}}\\|A \\mathbf{x}-\\mathbf{b}\\|\n",
        "$$\n",
        "\n",
        "In order to use the orthogonal decomposition result, we write\n",
        "$$\n",
        "A=\\left(\\begin{array}{ccc}\n",
        "\\mid & & \\mid \\\\\n",
        "\\mathbf{a}_{1} & \\ldots & \\mathbf{a}_{m} \\\\\n",
        "\\mid & & \\mid\n",
        "\\end{array}\\right)=\\left(\\begin{array}{ccc}\n",
        "a_{1,1} & \\cdots & a_{1, m} \\\\\n",
        "a_{2,1} & \\cdots & a_{2, m} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "a_{n, 1} & \\cdots & a_{n, m}\n",
        "\\end{array}\\right) \\quad \\text { and } \\quad \\mathbf{b}=\\left(\\begin{array}{c}\n",
        "b_{1} \\\\\n",
        "\\vdots \\\\\n",
        "b_{n}\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "\n",
        "Now we seek a linear combination of the columns of $A$ that minimizes the objective\n",
        "$$\n",
        "\\left\\|\\sum_{j=1}^{m} x_{j} \\mathbf{a}_{j}-\\mathbf{b}\\right\\|^{2}=\\sum_{i=1}^{n}\\left(\\sum_{j=1}^{m} x_{j} a_{i, j}-b_{i}\\right)^{2}=\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-b_{i}\\right)^{2}\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\hat{Y}_{i}=\\sum_{j=1}^{m} x_{j} a_{i j} .\n",
        "$$\n",
        "\n",
        "Now apply our characterization of the orthogonal projection on the column space of $A$. Let\n",
        "$$\n",
        "\\hat{\\mathbf{b}}=\\mathscr{P}_{\\operatorname{col}(A)} \\mathbf{b}\n",
        "$$\n",
        "\n",
        "Because $\\hat{\\mathbf{b}}$ is in the column space of $A$, the equation $A \\mathbf{x}=\\hat{\\mathbf{b}}$ is consistent and there is an $\\hat{x}$ such that\n",
        "$$\n",
        "\\begin{equation*}\n",
        "A \\hat{\\mathbf{x}}=\\hat{\\mathbf{b}} \\tag{1.3.1}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "Since $\\hat{\\mathbf{b}}$ is the closed point in $\\operatorname{col}(A)$ to $\\mathbf{b}$ a vector $\\hat{\\mathbf{x}}$ is a least-square solution of $A \\mathbf{x}=\\mathbf{b}$ if and only (1.3.1) holds. The following theorem provides an alternative description of the solution.\n",
        "\n",
        "**Theorem 1.3.1** (Normal equations) Let $A \\in \\mathbb{R}^{n \\times m}$ be an $n \\times m$ matrix with linearly independent columns and let $\\mathbf{b} \\in \\mathbb{R}^{n}$ be a vector. The solution to the least-squares problem\n",
        "$$\n",
        "\\min _{\\mathbf{x} \\in \\mathbb{R}^{m}}\\|A \\mathbf{x}-\\mathbf{b}\\|\n",
        "$$\n",
        "satisfies\n",
        "$$\n",
        "A^{T} A \\mathbf{x}=A^{T} \\mathbf{b}\n",
        "$$\n",
        "which is known as the normal equations.\n",
        "*Proof.* Let $U=\\operatorname{col}(A)=\\operatorname{span}\\left(\\mathbf{a}_{1}, \\ldots, \\mathbf{a}_{m}\\right)$. By the best approximation theorem, the orthogonal projection $\\hat{\\mathbf{b}}=A \\hat{\\mathbf{x}}$ of $\\mathbf{b}$ on $U$ is the unique solution to the least-squares problem. By the orthogonal decomposition, it must satisfy $\\langle\\mathbf{b}-\\hat{\\mathbf{b}}, \\mathbf{u}\\rangle=0$ for all $\\mathbf{u} \\in U$. Because the $\\mathbf{a}_{i}$ 's are a basis of $U$, it suffices that $\\left\\langle\\mathbf{b}-A \\hat{\\mathbf{x}}, \\mathbf{a}_{i}\\right\\rangle=0$ for all $i \\in\\{1, \\ldots, m\\}$. In matrix form,\n",
        "$$\n",
        "A^{T}(A \\hat{\\mathbf{x}}-\\mathbf{b})=\\mathbf{0}\n",
        "$$\n",
        "as claimed, after rearranging.\n",
        "When $A$ has linearly independent columns, in view of the QR decomposition, it can be shown that $A^{T} A$ is invertible and the solution of the normal equation is\n",
        "$$\n",
        "\\left(A^{T} A\\right)^{-1} A^{T} \\mathbf{b}\n",
        "$$\n",
        "\n",
        "However, that approach has numerical issues, we solve the problem via QR decomposition:\n",
        "- Construct an orthonormal basis of $\\operatorname{col}(A)$ through a QR decomposition:\n",
        "$$\n",
        "A=Q R\n",
        "$$\n",
        "- Form the orthogonal projection matrix,\n",
        "$$\n",
        "\\mathscr{P}_{\\operatorname{col}(A)}=\\mathrm{QQ}^{T}\n",
        "$$\n",
        "- Apply the projection to $\\mathbf{b}$ and observe that $\\mathbf{x}^{*}$ satisfies\n",
        "$$\n",
        "A \\mathbf{x}^{*}=\\mathrm{QQ}^{T} \\mathbf{b}\n",
        "$$\n",
        "- Use the QR decomposition for $A$ to get\n",
        "$$\n",
        "\\mathrm{QR} \\mathbf{x}^{*}=\\mathrm{QQ}^{T} \\mathbf{b}\n",
        "$$\n",
        "- Note $\\mathrm{Q}^{T} \\mathrm{Q}=I_{m \\times m}$ and multiply both sides by $\\mathrm{Q}^{T}$ to get\n",
        "$$\n",
        "R \\mathbf{x}^{*}=Q^{T} \\mathbf{b}\n",
        "$$\n",
        "- Because $R$ is upper triangular, solving this system for $\\mathbf{x}^{*}$ is straightforward. This is done via back substitution.\n",
        "\n",
        "**Theorem 1.3.2** (Least squares via QR ). Let $A \\in \\mathbb{R}^{n \\times m}$ be an $n \\times m$ matrix with linearly independent columns, let $\\mathbf{b} \\in \\mathbb{R}^{n}$ be a vector, and let $A=\\mathrm{QR}$ be a QR decomposition of $A$, where Q is a $\\mathbb{R}^{n \\times m}$ matrix with $\\mathrm{Q}^{T} \\mathrm{Q}=I_{m \\times m}$ and $R$ is upper triangular. The solution to the least-squares problem\n",
        "$$\n",
        "\\min _{\\mathbf{x} \\in \\mathbb{R}^{\\mathbb{R}^{\\prime}}}\\|A \\mathbf{x}-\\mathbf{b}\\|\n",
        "$$\n",
        "satisfies\n",
        "$$\n",
        "R \\mathbf{x}^{*}=Q^{T} \\mathbf{b}\n",
        "$$"
      ],
      "metadata": {
        "id": "cvLwGEGzbQ1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.3 Linear Regression"
      ],
      "metadata": {
        "id": "PmFf-W13cG1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given input data points $\\left\\{\\left(\\mathbf{x}_{i}, y_{i}\\right)\\right\\}_{i=1}^{n}$ with each $\\mathbf{x}_{i}=\\left(x_{i 1}, \\ldots, x_{i d}\\right)^{T}$, we seek an affine function to fit the data. The common approach involves finding coefficients $\\beta_{j}$ 's that minimize the criterion\n",
        "$$\n",
        "\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\hat{y}_{i}=\\beta_{0}+\\sum_{j=1}^{d} \\beta_{j} x_{i j}\n",
        "$$\n",
        "can be viewed as the predicted values of the linear model with coefficients $\\beta_{j}$. The minimization problem can be formulated in matrix form. Let\n",
        "$$\n",
        "\\mathbf{y}=\\left(\\begin{array}{c}\n",
        "y_{1} \\\\\n",
        "y_{2} \\\\\n",
        "\\vdots \\\\\n",
        "y_{n}\n",
        "\\end{array}\\right), \\quad A=\\left(\\begin{array}{cc}\n",
        "1 & \\mathbf{x}_{1}^{T} \\\\\n",
        "1 & \\mathbf{x}_{2}^{T} \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & \\mathbf{x}_{n}^{T}\n",
        "\\end{array}\\right) \\quad \\text { and } \\quad \\boldsymbol{\\beta}=\\left(\\begin{array}{c}\n",
        "\\beta_{0} \\\\\n",
        "\\beta_{1} \\\\\n",
        "\\vdots \\\\\n",
        "\\beta_{d}\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "\n",
        "Then the problem is transformed to\n",
        "$$\n",
        "\\min _{\\beta}\\|\\mathbf{y}-A \\boldsymbol{\\beta}\\|^{2}\n",
        "$$\n",
        "\n",
        "This is exactly the least-squares problem that we discuss in the last section."
      ],
      "metadata": {
        "id": "Hk47Tj6TcKnM"
      }
    }
  ]
}