{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPDpmz+1Ld25dSv5o0y8aVI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JadonTelep/MAT-422/blob/main/SP_24_MAT_422_1_2_Elements_of_Linear_Algebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 1 - Linear Algebra**"
      ],
      "metadata": {
        "id": "R9yCJxy2g5Wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Introduction"
      ],
      "metadata": {
        "id": "85ZvI6fYhBh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear algebra is a field of mathematics that is widely used in various disciplines. Linear algebra plays an important role in data science and machine\n",
        "learning. A solid understanding of linear algebra concepts can enhance the\n",
        "understanding of many data science and machine learning algorithms. This\n",
        "chapter introduces basic concepts of linear algebra that need data science\n",
        "including vector spaces, orthogonality, eigenvalues, matrix decomposition,\n",
        "and is further expanded to include linear regression and principal component analysis where linear algebra plays a central role for solving data science\n",
        "problems. More advanced concepts and applications of linear algebra can\n",
        "be found in many references"
      ],
      "metadata": {
        "id": "8I7ZiztJhG5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Elements of Linear Algebra"
      ],
      "metadata": {
        "id": "bO2cJyEchJgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Linear Spaces"
      ],
      "metadata": {
        "id": "L6cz0ywzhPen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1.1 Linear Combinations"
      ],
      "metadata": {
        "id": "6x5x0dQbnZ07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A linear combination in linear algebra is a new vector constructed from\n",
        "a subset by multiplying each vector by a constant and adding the results.\n",
        "A linear subspace is a subset of all linear combination.\n",
        "\n",
        "**Definition 1.2.1 (Linear subspace).** A linear subspace of $V$ is a subset\n",
        "$U ⊆ V$ that is closed under vector addition and scalar multiplication. That\n",
        "is, for all $u_1,u_2 ∈ U$ and $α ∈ R$, it holds that\n",
        "\n",
        "$$ u _1+ u _2 \\in U \\quad and \\quad \\alpha  u _1 \\in U.$$\n",
        "\n",
        "In particular, **0** is always in a linear subspace. As we can see below, a\n",
        "span of a set of vectors is a linear subspace."
      ],
      "metadata": {
        "id": "eUD5YHe0hSYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 1.2.2 (Span).** Let $w_1,...,w_m ∈ V$. The span of $\\{w_1,...,w_m\\}$, denoted span$(w1,...,wm)$, is the set of all linear combinations of the $w_j$'s.\n",
        "That is,\n",
        "\n",
        "$$span(w_1,...,w_m) = \\{ \\sum_{j=1}^{m} \\alpha_jw_j: \\alpha_1,...,\\alpha_m \\in ℝ\\}$$\n",
        "\n",
        "A list of vectors that span a linear subspace U is also referred to as a\n",
        "spanning set of U. We can verify that a span is a linear subspace."
      ],
      "metadata": {
        "id": "8D3hnO6AkEKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemma 1.2.3 (Every span is a linear subspace).** Let $W = $span$(w1,...,wm)$.\n",
        "Then $W$ is a linear subspace.\n",
        "\n",
        "*Proof*. Let $u1,u2 ∈ W$, and $α ∈ R$. Then for $i = 1, 2,$\n",
        "\n",
        "$$\\sum_{j=1}^{m} β_{i,j}w_j,$$\n",
        "\n",
        "and\n",
        "\n",
        "$$αu_1 + u_2 = α\\sum_{j=1}^{m} β_{1,j}w_j + \\sum_{j=1}^{m} β_{2,j}w_j = \\sum_{j=1}^{m} (αβ_{1,j} + β_{2,j})w_j.$$\n",
        "\n",
        "We conclude that $αu_1 + u_2 \\in W$.\n",
        "\n",
        "Often it is useful to study the column space of a matrix.\n",
        "\n",
        "**Definition 1.2.4 (Column space).** Let $A ∈ R^{n \\times m}$ be an $n \\times m$ matrix with columns $a1,..., am ∈ R^n$. The column space of $A$, denoted col$(A)$, is the span of the columns of $A$, that is, col$(A)$ = span$(a1,..., am) ∈ R^n$.\n"
      ],
      "metadata": {
        "id": "XeqbIHzikrqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1.2 Linear Independence And Dimension"
      ],
      "metadata": {
        "id": "oZ2gnEiKnf-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For many problems in applications including data science, it is desirable to\n",
        "avoid redundancy in the description of a linear subspace. The concept is\n",
        "central to the definition of dimension of a linear space.\n",
        "\n",
        "**Definition 1.2.5 (Linear independence).** A list of vectors $u_1,...,u_m$ is\n",
        "linearly independent if none of them can be written as a linear combination\n",
        "of the others, that is\n",
        "\n",
        "$$∀i, u_i ∉span(\\{u_j:j\\neq i\\})$$\n",
        "\n",
        "**Lemma 1.2.6.** The vectors $u_1,...,u_m$ are linearly independent if and only if\n",
        "\n",
        "$$\\sum_{j=1}^{m} \\alpha_ju_j = 0 ⟹ \\alpha_j = 0, ∀j$$\n",
        "\n",
        "A list of vectors is called linearly dependent if it is not linearly independent.\n",
        "\n",
        "Equivalently, $u_1,...,u_m$ are linearly dependent if and only if there exist $αj'$s, not all zero, such that\n",
        "\n",
        "$\\sum_{j=1}^{m} \\alpha_ju_j = 0$."
      ],
      "metadata": {
        "id": "9QE4e7xO3NOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Proof.* The equivalence follows by contradiction. We prove the second\n",
        "statement. Assume $u_1,...,u_m$ are linearly dependent. Then $u_i=\\sum_{j\\neq i} \\alpha_ju_j$ for some $i$. Taking $α_i = -1$ gives $\\sum_{j=1}^{m} \\alpha_ju_j = 0$ On the other hand, assume $\\sum_{j=1}^{m} \\alpha_ju_j = 0$ with $α_j's$ not all zero. In particular, $α_i \\neq 0$ for some $i$. Then $u_i = \\frac{1}{\\alpha_j}\\sum_{j\\neq i} \\alpha_ju_j$.\n",
        "\n",
        "For matrix form, let $a_1,..., a_m ∈ R^n$ and\n",
        "\n",
        "$$A=\\left(\\begin{array}{ccc}\\mid & & \\mid \\\\  a _1 & \\ldots &  a _m \\\\ \\mid & & \\mid\\end{array}\\right)$$\n",
        "\n",
        "Then linearly independence can be formulated as if there is a non-trivial\n",
        "solution of a linear system. It is clear that $Ax$ is the following linear\n",
        "combination of the columns of $A:\\sum_{j=1}^{m} x_ja_j$ Then $a_1,...,a_m$ are linearly\n",
        "\n",
        "independent if and only if $Ax = 0 =⇒ x = 0$. Equivalently, a1,..., am are\n",
        "linearly dependent if and only if $∃x \\neq 0$ such that $Ax = 0$. We now look at the concept of bases, which give a minimal representation of a subspace. A basis is a set of vectors that generates all elements of the vector space and the vectors in the set are linearly independent.\n"
      ],
      "metadata": {
        "id": "XlpnGSK74cjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 1.2.7 (Basis of a space).** Let U be a linear subspace of $V$.\n",
        "A basis of $U$ is a list of vectors $u_1,...,u_m$ in $U$ that:\n",
        "\n",
        "1.   Span $U$, that is, $U = $span$(u_1,...,u_m)$\n",
        "2.   Are linearly independent\n",
        "\n",
        "We denote by $e_1,...,e_n$ the standard basis of $R^n$, where $e_i$ has a one\n",
        "in coordinate $i$ and zeros in all other coordinates. One of the first key\n",
        "properties of a basis is that it provides a unique representation of the vectors in the subspace. Indeed, let U be a linear subspace and $u_1,...,u_m$ be\n",
        "a basis of $U$. Suppose that $w ∈ U$ can be written as $w = \\sum_{j=1}^{m} α_ju_j$ and $w = \\sum_{j=1}^{m} α'_ju_j$. Then subtracting one equation from the other we arrive at $w = \\sum_{j=1}^{m} (α_j-α'_j)u_j$. By linear independence, we have $α_j-α'_j = 0$ for each $j = 0$.\n",
        "\n",
        "A vector space can have several bases; however, all of the bases have the\n",
        "same number of elements, called the dimension of the vector space. When\n",
        "applied to a matrix $A$, the dimension of the column space of $A$ is called the\n",
        "(column) rank of $A$. We state it as the following theorem."
      ],
      "metadata": {
        "id": "M5q2u0oP7nzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theorem 1.2.8 (Dimension theorem).** Let $U$ be a linear subspace of $V$. Any\n",
        "basis of $U$ always has the same number of elements. All bases of $U$ have the same length, that is, the same number of elements. We call this number the dimension of $U$ and denote it dim$(U)$.\n",
        "\n",
        "The following lemma further describes the property of a linearly dependent set, which is used to prove the dimension theorem. It states that,\n",
        "given a linearly dependent list of vectors, one of the vectors is in the span\n",
        "of the previous ones and we can remove it without changing the span.\n",
        "\n",
        "**Lemma 1.2.9 (Characterization of linearly dependent sets).** Let $u_1,...,u_m$\n",
        "be a linearly dependent list of vectors with a linearly independent subset, $u_i$, $i ∈ \\{1,..., k\\}$, $k < m$. Then there is an $i > k$ such that:\n",
        "\n",
        "1.   $u_i ∈ $span$(u_1,...,u_{i−1});$\n",
        "2.   span$({u_j : j ∈ {1,...,m}}) = $span$({u_j : j ∈ {1,...,m}, j \\neq i})$.\n",
        "\n",
        "*Proof (Characterization of linearly dependent sets)*. For 1, by linear dependence, $\\sum_{j=1}^{m} \\alpha_ju_j = 0$ with not all $αj's$ zero. Further, because $u_i$, $i ∈ \\{1,..., k\\}$, $k < m$ is independent, and not all $α_{k+1},...,α_m$ are zero. Take the largest index among the $α_j%'s that are non-zero, say $i$. Then rearranging gives\n",
        "\n",
        "$$u_i=-\\sum_{j=1}^{i-1} \\frac{α_j}{α_i}u_j = 0.$$\n",
        "\n",
        "For 2, we note that for any $w ∈ $span$({u_j : j ∈ {1,...,m}})$ we can write\n",
        "it as $w = \\sum_{j=1}^{m} β_{i,j}u_j$, and we can replace $u_i$ by the equation above, producing a representation of $w$ in terms of ${u_j : j ∈ {1,...,m},j \\neq i}$."
      ],
      "metadata": {
        "id": "QFRqyIKN9Rhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now able to prove the dimension theorem.\n",
        "\n",
        "*Proof (Dimension theorem).* Suppose we have two bases ${w_i : i ∈ {1,..., n}}$\n",
        "and ${u_j : j ∈ {1,...,m}}$ of $U$. It suffices to show that $n ≥ m$. First, we consider the list ${u_1,w_1,...,w_n}$. Because the $w_i$'s are spanning, adding $u1 \\neq 0$ to them necessarily produces a linearly dependent list. By the lemma for the characterization of linearly dependent sets, we can remove one of the $w_i$'s without changing the span. The new list $B$ has length $n$ again. Then we add $u_2$ to $B$ immediately after $u_1$. By the lemma for the characterization of linearly dependent sets, one of the vectors in this list is in the span of the previous ones. It cannot be $u_2$ as $\\{u_1,u_2\\}$ are linearly independent by assumption. So it must be one of the remaining $w_i$'s. We remove that one, without changing the span by the linear dependence lemma again. This process can be continued until we have added all the $uj$'s, as otherwise a subset of ${u_j : j ∈ {1,...,m}}$ would span $U$, which is a contradiction. Hence $n ≥ m$."
      ],
      "metadata": {
        "id": "qDr7688j_Q4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Orthogonality"
      ],
      "metadata": {
        "id": "CqwVVqKY_8jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many applications, the use of orthonormal bases can greatly simplify\n",
        "mathematical representations and reveal more insights of the underlying\n",
        "problems. We begin with the following definitions and lemmas."
      ],
      "metadata": {
        "id": "Lc8yieIfFmu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2.1 Orthonormal Bases"
      ],
      "metadata": {
        "id": "XhcPM7jeFoTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 1.2.10 (Norm and inner product).**\n",
        "\n",
        "$\\langle u, v \\rangle = u \\cdot v=\\sum_i^n u_i v_i$ and $\\|u\\|=\\sqrt{\\sum_1^n u_i^2}$\n",
        "\n",
        "**Definition 1.2.11** A list of vectors $\\{u_1,...,um\\}$ is orthonormal if the $u_i$'s are pairwise orthogonal and each has norm 1, that is, for all $i$ and all $j \\neq i$, $\\langle u_i, v_j \\rangle = 0$, and $\\|u\\| = 1$.\n",
        "\n",
        "We generalize the Pythagorean theorem with orthogonal vectors.\n",
        "\n",
        "**Lemma 1.2.12 (Pythagorean theorem).** Let $u,v ∈ V$ be orthogonal. Then\n",
        "\\|u + v\\|^2 = \\|u\\|^2 + \\|v\\|^2.\n",
        "\n",
        "*Proof (Pythagorean theorem).* Using $\\|w\\|^2 = \\langle w, w \\rangle$ we get\n",
        "\n",
        "$$\n",
        "\\| u + v \\|^2=\\langle u + v ,  u + v \\rangle=\\langle u ,  u \\rangle+2\\langle u ,  v \\rangle+\\langle v ,  v \\rangle=\\| u \\|^2+\\| v \\|^2\n",
        "$$\n",
        "\n",
        "Many useful results can be derived from Pythagorean theorem, for example, we have the following.\n",
        "\n",
        "**Lemma 1.2.13 (Cauchy-Schwarz).** For anr $ u ,  v  \\in V,|\\langle u ,  v \\rangle| \\leq\\| u \\|\\| v \\|$."
      ],
      "metadata": {
        "id": "NRK1XDIHFrxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Proof (Cauchy-Schwarz).* Let $ q =\\frac{ v }{| v |}$ be the unit vector in the direction of $ v $. We want to show $|\\langle u ,  q \\rangle| \\leq\\| u \\|$. Decompose $ u $ into its projection onto $ q $ and what is left is the following:\n",
        "\n",
        "$$\n",
        " u =\\langle u ,  q \\rangle  q +\\{ u -\\langle u ,  q \\rangle  q \\}\n",
        "$$\n",
        "\n",
        "The two terms on the right-hand side are orthogonal, so the Pythagorean theorem gives\n",
        "\n",
        "$$\n",
        "\\| u \\|^2=\\|\\langle u ,  q \\rangle  q \\|^2+\\| u -\\langle u ,  q \\rangle  q \\|^2 \\geq\\|\\langle u ,  q \\rangle  q \\|^2=\\langle u ,  q \\rangle^2\n",
        "$$\n",
        "\n",
        "Taking a square root gives the claim.\n",
        "\n",
        "Now we have the following properties for orthonormal lists.\n",
        "\n",
        "**Lemma 1.2.14.** Let $\\left\\{ u _1, \\ldots,  u _m\\right\\}$ be an *orthonormal list of vectors.*\n",
        "\n",
        "1. $\\left\\|\\sum_{j=1}^m \\alpha_j  u _j\\right\\|^2=\\sum_{j=1}^m \\alpha_j^2$ for any $\\alpha_j \\in \\mathbb{R}, j \\in\\{1, \\ldots, m\\}$;\n",
        "2. $\\left\\{ u _1, \\ldots,  u _m\\right\\}$ are *linearly independent*.\n",
        "\n",
        "*Proof.* For 1, noting that $\\| x \\|^2=\\langle x ,  x \\rangle$ and $\\left\\langle\\beta  x _1+ x _2,  x _3\\right\\rangle=\\beta\\left\\langle x _1,  x _3\\right\\rangle+$ $\\left\\langle x _2,  x _3\\right\\rangle$, we have\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\left\\|\\sum_{j=1}^m \\alpha_j  u _j\\right\\|^2 & =\\left\\langle\\sum_{i=1}^m \\alpha_i  u _i, \\sum_{j=1}^m \\alpha_j  u _j\\right\\rangle=\\sum_{i=1}^m \\alpha_i\\left\\langle u _i, \\sum_{j=1}^m \\alpha_j  u _j\\right\\rangle \\\\\n",
        "& =\\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j\\left\\langle u _i,  u _j\\right\\rangle\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "which $\\sum_{i=1}^m \\alpha_i^2$, where we used orthonormality in the rightmost equation, that is, $\\left\\langle u _i,  u _j\\right\\rangle$ is 1 if $i=j$ and 0 otherwise."
      ],
      "metadata": {
        "id": "dlVo4nCoIoFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For 2 , suppose $\\sum_{i=1}^m \\beta_i  u _i= 0 $. Then we must have by 1 that $\\sum_{i=1}^m \\beta_i^2=0$. That implies $\\beta_i=0$ for all $i$. Hence the $ u _i$ 's are linearly independent.\n",
        "\n",
        "Given a basis $\\left\\{ u _1, \\ldots,  u _m\\right\\}$ of a subspace $\\mathscr{U}$, we know that: for any $ w  \\in$ $\\mathscr{U},  w =\\sum_{i=1}^m \\alpha_i  u _i$ for some $\\alpha_i$ 's. It is not immediately obvious in general how to find these $\\alpha_i$ 's. In the orthonormal case, it is straightforward.\n",
        "\n",
        "**Theorem 1.2.15 (Orthonormal basis expansion).** Let $ q _1, \\ldots,  q _m$ be an orthonormal basis of $\\mathscr{U}$ and let $ u  \\in \\mathscr{U}$. Then\n",
        "$$\n",
        " u =\\sum_{j=1}^m\\left\\langle u ,  q _j\\right\\rangle  q _j\n",
        "$$\n",
        "\n",
        "*Proof.* Because $ u  \\in \\mathscr{U},  u =\\sum_{i=1}^m \\alpha_i  q _i$ for some $\\alpha_i$. Take the inner product with $ q _j$ and use orthonormality:\n",
        "$$\n",
        "\\left\\langle u ,  q _i\\right\\rangle=\\left\\langle\\sum_{i=1}^m \\alpha_i  q _i,  q _j\\right\\rangle=\\sum_{i=1}^m \\alpha_i\\left\\langle q _i,  q _j\\right\\rangle=\\alpha_j\n",
        "$$"
      ],
      "metadata": {
        "id": "WcXD7WQUJK8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2.2 Best Approcimation Theorem"
      ],
      "metadata": {
        "id": "6ezDOi9yPcfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many optimization applications can be converted to the following best approximation problem. We have a linear subspace $\\mathscr{U} \\subseteq V$ and a vector $ v  \\notin \\mathscr{U}$. We want to find the vector $ v ^*$ in $\\mathscr{U}$ that is closest to $ v $ in the norm as in **Fig. 1.1**, that is, we want to solve\n",
        "$$\n",
        "\\min _{ v ^* \\in \\mathscr{U}}\\left\\| v ^*- v \\right\\| .\n",
        "$$\n",
        "\n",
        "**Example 1.2.16.** Consider the two-dimensional case with a one-dimensional subspace, say $\\mathscr{U}=$span$\\left( u _1\\right)$ with $\\left\\| u _1\\right\\|=1$. The geometrical intuition of the best approximation theorem is demonstrated in **Fig. 1.1**. The solution $ v ^*$ has the property that the difference $ v - v ^*$ makes a right angle with $ u _1$, that is, it is orthogonal to it.\n",
        "\n",
        "Letting $ v ^*=\\alpha^*  u _1$, the geometrical condition above translates into\n",
        "$$\n",
        "0=\\left\\langle u _1,  v - v ^*\\right\\rangle=\\left\\langle u _1,  v -\\alpha^*  u _1\\right\\rangle=\\left\\langle u _1,  v \\right\\rangle-\\alpha^*\\left\\langle u _1,  u _1\\right\\rangle=\\left\\langle u _1,  v \\right\\rangle-\\alpha^*\n",
        "$$\n",
        "which implies that\n",
        "$$\n",
        " v ^*=\\left\\langle u _1,  v \\right\\rangle  u _1\n",
        "$$"
      ],
      "metadata": {
        "id": "4pmCnxX5PmDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2024-09-01 125915.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "zNQIeDQ2KYAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the Pythagorean theorem, we then have for any $\\alpha \\in \\mathbb{R}$,\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\left\\| v -\\alpha  u _1\\right\\|^2 & =\\left\\| v - v ^*+ v ^*-\\alpha  u _1\\right\\|^2=\\left\\| v - v ^*+\\left(\\alpha^*-\\alpha\\right)  u _1\\right\\|^2 \\\\\n",
        "& =\\left\\| v - v ^*\\right\\|^2+\\left\\|\\left(\\alpha^*-\\alpha\\right)  u _1\\right\\|^2\n",
        "\\end{aligned}\n",
        "$$\n",
        "and, therefore,\n",
        "$$\n",
        "\\left\\| v -\\alpha  u _1\\right\\|^2 \\geq\\left\\| v - v ^*\\right\\|^2\n",
        "$$\n",
        "\n",
        "This confirms the optimality of $ v ^*$.\n",
        "The argument in the example above carries through in higher dimension, leading to the following fundamental result.\n",
        "\n",
        "**Definition 1.2.17 (Orthogonal projection).** Let $\\mathscr{U} \\subseteq V$ be a linear subspace with orthonormal basis $ q _1, \\ldots,  q _m$. The orthogonal projection of $ v  \\in V$ on $\\mathscr{U}$ is defined as\n",
        "$$\n",
        "\\mathscr{P}_{\\mathscr{U}}  v =\\sum_{j=1}^m\\left\\langle v ,  q _j\\right\\rangle  q _j\n",
        "$$\n",
        "\n",
        "**Theorem 1.2.18 (Best approximation theorem).** Let $\\mathscr{U} \\subseteq V$ be a linear subspace with orthonormal basis $ q _1, \\ldots,  q _m$ and let $ v  \\in V$. For any $ u  \\in \\mathscr{U}$,\n",
        "$$\n",
        "\\left\\| v -\\mathscr{P}_{\\mathscr{U}}  v \\right\\| \\leq\\| v - u \\|\n",
        "$$\n",
        "\n",
        "Furthermore, if $ u  \\in \\mathscr{U}$ and the inequality above is an equality, then $ u =\\mathscr{P}_{\\mathscr{U}}  v $.\n",
        "The visualization of the theorem is shown in **Fig. 1.2**. In addition, we note the following.\n",
        "\n",
        "**Lemma 1.2.19 (Orthogonal decomposition).** Let $\\mathscr{U} \\subseteq V$ be a linear subspace with orthonormal basis $ q _1, \\ldots,  q _m$ and let $ v  \\in V$. For any $ u  \\in \\mathscr{U}$, $\\left\\langle v -\\mathscr{P}_{\\mathscr{U}}  v ,  u \\right\\rangle=0$. In particular, $ v $ can be decomposed as $\\left( v -\\mathscr{P}_{\\mathscr{U}}  v \\right)+\\mathscr{P}_{\\mathscr{U}}  v $ where the two terms are orthogonal."
      ],
      "metadata": {
        "id": "Li5H09DyJ7FY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2024-09-01 130050.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "0EnXg0kZLT-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Proof (Orthogonal decomposition).* We can write any $ u  \\in \\mathscr{U}$ as $\\sum_{j=1}^m \\alpha_j  q _j$ for some $\\alpha_j$ 's. Then\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\left\\langle v -\\mathscr{P}_{U}  v ,  u \\right\\rangle & =\\left\\langle v -\\sum_{j=1}^m\\left\\langle v ,  q _j\\right\\rangle  q _j, \\sum_{j=1}^m \\alpha_j^{\\prime}  q _j\\right\\rangle \\\\\n",
        "& =\\sum_{j=1}^m\\left\\langle v ,  q _j\\right\\rangle \\alpha_j^{\\prime}-\\sum_{j=1}^m \\alpha_j^{\\prime}\\left\\langle v ,  q _j\\right\\rangle=0\n",
        "\\end{aligned}\n",
        "$$\n",
        "where we used the orthonormality of the $ q _j$ 's in the rightmost equality. The second claim follows from $\\mathscr{P}_{\\mathscr{U}}  v  \\in \\mathscr{U}$.\n",
        "\n",
        "We return to the proof of our main theorem.\n",
        "\n",
        "*Proof (Best approximation theorem).* For any $ u  \\in \\mathscr{U}$, the vector $ u ^{\\prime}=\\mathscr{P}_{\\mathscr{U}}  v - u $ is also in $\\mathscr{U}$. By the orthogonal decomposition lemma and Pythagoras,\n",
        "\n",
        "Furthermore, equality holds only if $\\left\\|\\mathscr{P}_{\\mathscr{U}}  v - u \\right\\|^2=0$, which holds only if $ u =\\mathscr{P}_{\\mathscr{U}}  v $ by the point-separating property of the norm.\n",
        "\n",
        "The map $\\mathscr{P}_{\\mathscr{U}}$ is linear, that is, $\\mathscr{P}_{\\mathscr{U}}(\\alpha  x + y )=\\alpha \\mathscr{P}_{\\mathscr{U}}  x +\\mathscr{P}_{\\mathscr{U}}  y $ for all $\\alpha \\in \\mathbb{R}$ and $ x ,  y  \\in \\mathbb{R}^n$. Indeed,\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathscr{P}_{\\mathscr{U}}(\\alpha  x + y ) & =\\sum_{j=1}^m\\left\\langle\\alpha  x + y ,  q _j\\right\\rangle  q _j=\\sum_{j=1}^m\\left\\{\\alpha\\left\\langle x ,  q _j\\right\\rangle+\\left\\langle y ,  q _j\\right\\rangle\\right\\}  q _j \\\\\n",
        "& =\\alpha \\mathscr{P}_{\\mathscr{U}}  x +\\mathscr{P}_{\\mathscr{\\mathscr { M }}}  y\n",
        "\\end{aligned}\n",
        "$$\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\| v - u \\|^2=\\left\\| v -\\mathscr{P}_{\\mathscr{2}}  v +\\mathscr{P}_{\\mathscr{2}}  v - u \\right\\|^2 \\\\\n",
        "& =\\left\\| v -\\mathscr{P}_{\\mathscr{U}}  v \\right\\|^2+\\left\\|\\mathscr{P}_{\\mathscr{U}}  v - u \\right\\|^2 \\geq\\left\\| v -\\mathscr{P}_{\\mathscr{U}}  v \\right\\|^2 .\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "wrwKOAa-LfgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result, it can be encoded as an $n \\times m$ matrix $Q$. Let\n",
        "$$\n",
        "Q=\\left(\\begin{array}{ccc}\n",
        "\\mid & & \\mid \\\\\n",
        " q _1 & \\ldots &  q _m \\\\\n",
        "\\mid & & \\mid\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "and note that computing\n",
        "$$\n",
        "Q^T  v =\\left(\\begin{array}{c}\n",
        "\\left\\langle v ,  q _1\\right\\rangle \\\\\n",
        "\\cdots \\\\\n",
        "\\left\\langle v ,  q _m\\right\\rangle\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "lists the coefficients in the expansion of $\\mathscr{P}_{\\mathscr{U}}  v $ over the basis $ q _1, \\ldots,  q _m$. Hence we see that\n",
        "$$\n",
        "\\mathscr{P}=\\mathrm{QQ}^T .\n",
        "$$\n",
        "\n",
        "On the other hand,\n",
        "$$\n",
        "Q^T Q=\\left(\\begin{array}{ccc}\n",
        "\\left\\langle q _1,  q _1\\right\\rangle & \\cdots & \\left\\langle q _1,  q _m\\right\\rangle \\\\\n",
        "\\left\\langle q _2,  q _1\\right\\rangle & \\cdots & \\left\\langle q _2,  q _m\\right\\rangle \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\left\\langle q _m,  q _1\\right\\rangle & \\cdots & \\left\\langle q _m,  q _m\\right\\rangle\n",
        "\\end{array}\\right)=I_{m \\times m}\n",
        "$$\n",
        "where $I_{m \\times m}$ denotes the $m \\times m$ identity matrix."
      ],
      "metadata": {
        "id": "HdNJquxbL9WU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2.3 Gram-Schmidt process"
      ],
      "metadata": {
        "id": "M5TGHHEKMYtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gram-Schmidt algorithm is used to obtain an orthonormal basis. Let $ a _1, \\ldots,  a _m$ be linearly independent. We intend to find an orthonormal basis of span$\\left( a _1, \\ldots,  a _m\\right)$. The process takes advantage of the properties of the orthogonal projection derived above. In essence, we add the vectors $ a _i$ one by one, but only after taking out their orthogonal projection on the previously included vectors. The outcome spans the same subspace and orthogonal decomposition ensures orthogonality.\n",
        "\n",
        "**Theorem 1.2.20 (Gram-Schmidt).** Let $ a _1, \\ldots,  a _m$ in $R^n$ *be linearly independent. Then there exist an orthonormal basis* $ q _1, \\ldots,  q _m$ of span$\\left( a _1, \\ldots,  a _m\\right)$.\n",
        "\n",
        "*Proof.* The inductive step is the following. Assume that we have constructed orthonormal vectors $ q _1, \\ldots,  q _{\\text {i-1 }}$ such that\n",
        "$\n",
        "U_{i-1}:=$span$\\left( q _1, \\ldots,  q _{i-1}\\right)=$span$\\left( a _1, \\ldots,  a _{i-1}\\right)\n",
        "$"
      ],
      "metadata": {
        "id": "0mHHBVC1MZV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have an orthonormal basis for $U_{i-1}$, we can compute the orthogonal projection of $ a _i$,\n",
        "$$\n",
        "\\mathscr{P}_{U_{i-1}}  a _i=\\sum_{j=1}^{i-1}\\left\\langle a _i,  q _j\\right\\rangle  q _j\n",
        "$$\n",
        "\n",
        "And we set\n",
        "$$\n",
        " b _i= a _i-\\mathscr{P}_{U_{i-1}}  a _i \\quad \\text { and } \\quad  q _i=\\frac{ b _i}{\\left\\| b _i\\right\\|}\n",
        "$$\n",
        "\n",
        "Here, we used that $\\left\\| b _i\\right\\|>0$; otherwise, $ a _i$ would be equal to its projection $\\mathscr{P}_{U_{i-1}}  a _i \\in $span$\\left( a _1, \\ldots,  a _{i-1}\\right)$, which would contradict linear independence of the $ a _j$ 's. By the orthogonal decomposition result, $ q _i$ is orthogonal to span$\\left( q _1, \\ldots,  q _{i-1}\\right)$ and, unrolling the calculations above, $ a _i$ is the following linear combination of $ q _1, \\ldots,  q _i$ :\n",
        "$$\n",
        " a _i=\\sum_{j=1}^{i-1}\\left\\langle a _i,  q _i\\right\\rangle  q _j+\\left\\| a _i-\\sum_{j-1}^{i-1}\\left\\langle a _i,  q _i\\right\\rangle  q _j\\right\\|  q _i\n",
        "$$\n",
        "\n",
        "Hence $ q _1, \\ldots,  q _i$ forms an orthonormal list with span$\\left( a _1, \\ldots,  a _i\\right) \\subseteq$ span$\\left( q _1, \\ldots,  q _i\\right)$. The opposite inclusion holds by construction. Moreover, because $ q _1, \\ldots,  q _i$ are orthonormal, they are linearly independent so we must form a basis of their span so that induction goes through.\n"
      ],
      "metadata": {
        "id": "MFDaxB-5MxAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.4 Eigenvalues and eigenvectors"
      ],
      "metadata": {
        "id": "oqUr82QKOfD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvalues and eigenvectors are key concepts in many applications. As before, we work on $\\mathbb{R}^d$.\n",
        "\n",
        "**Definition 1.2.21 (Eigenvalues and eigenvectors).** Let $A \\in \\mathbb{R}^{d \\times d}$ be a square matrix. Then $\\lambda \\in \\mathbb{R}$ is an eigenvalue of $A$ if there exists a non-zero vector $ x  \\neq  0 $ such that\n",
        "$$\n",
        "A  x =\\lambda  x\n",
        "$$\n",
        "\n",
        "The vector $ x $ is referred to as an eigenvector.\n",
        "As the next example shows, not every matrix has an eigenvalue.\n",
        "\n",
        "**Example 1.2.22 (No real eigenvalues).** Set $d=2$ and let\n",
        "$$\n",
        "A=\\left(\\begin{array}{cc}\n",
        "0 & -1 \\\\\n",
        "1 & 0\n",
        "\\end{array}\\right)\n",
        "$$"
      ],
      "metadata": {
        "id": "rwHkZCcxM9wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For $\\lambda$ to be an eigenvalue, there must be an non-zero eigenvector $ x =$ $\\left(x_1, x_2\\right)^T$ such that\n",
        "$$\n",
        "A  x =\\lambda  x\n",
        "$$\n",
        "or put differently\n",
        "$$\n",
        "-x_2=\\lambda x_1 \\quad \\text { and } \\quad x_1=\\lambda x_2\n",
        "$$\n",
        "\n",
        "Replacing these equations into each other, it must be that\n",
        "$$\n",
        "-x_2=\\lambda^2 x_2 \\quad \\text { and } \\quad x_1=-\\lambda^2 x_1\n",
        "$$\n",
        "\n",
        "Because $x_1, x_2$ cannot both be $0, \\lambda$ must satisfy the equation\n",
        "$$\n",
        "\\lambda^2=-1\n",
        "$$\n",
        "for which there is no real solution.\n",
        "As we can see from below, $A \\in \\mathbb{R}^{d \\times d}$ has at most $d$ distinct eigenvalues.\n",
        "\n",
        "**Lemma 1.2.23 (Number of eigenvalues).** Let $A \\in \\mathbb{R}^{d \\times d}$ and let $\\lambda_1, \\ldots, \\lambda_m$ be distinct eigenvalues of $A$ with corresponding non-zero eigenvectors $ x _1, \\ldots,  x _m$. Then $ x _1, \\ldots,  x _m$ are linearly independent. As a result, $m \\leq d$.\n",
        "\n",
        "Proof. Assume by contradiction that $ x _1, \\ldots,  x _m$ are linearly dependent. By linear dependence, there is $k \\leq m$ such that\n",
        "$$\n",
        " x _k \\in \\operatorname{span}\\left( x _1, \\ldots,  x _{k-1}\\right)\n",
        "$$\n",
        "where $ x _1, \\ldots,  x _{k-1}$ are linearly independent. In particular, there are $a_1, \\ldots, a_{k-1}$ such that\n",
        "$$\n",
        " x _k=a_1  x _1+\\cdots+a_{k-1}  x _{k-1}\n",
        "$$\n",
        "\n",
        "Transform the equation above in two ways: (1) multiply both sides by $\\lambda_k$ and (2) apply $A$. Then subtract the resulting equations. That leads to\n",
        "$$\n",
        " 0 =a_1\\left(\\lambda_k-\\lambda_1\\right)  x _1+a_{k-1}\\left(\\lambda_k-\\lambda_{k-1}\\right)  x _{k-1}\n",
        "$$\n",
        "\n",
        "Because the $\\lambda_i$ 's are distinct and $ x _1, \\ldots,  x _{k-1}$ are linearly independent, we must have $a_1=\\cdots=a_{k-1}=0$. But that implies that $ x _k= 0 $, a contradiction. For the second claim, if there were more than $d$ distinct eigenvalues, then there would be more than $d$ corresponding linearly independent eigenvectors by the first claim, a contradiction."
      ],
      "metadata": {
        "id": "E5DG7WCGOrcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.4.1 Diagonalization of symmetric matrices"
      ],
      "metadata": {
        "id": "iFYoofDSRlHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the notation $\\operatorname{diag}\\left(\\lambda_1, \\ldots, \\lambda_d\\right)$ for the diagonal matrix with diagonal entries $\\lambda_1, \\ldots, \\lambda_d$.\n",
        "\n",
        "**Example 1.2.24 (Diagonal (and similar) matrices).** Let $A$ be similar to a matrix $D=\\operatorname{diag}\\left(\\lambda_1, \\ldots, \\lambda_d\\right)$ with distinct diagonal entries, that is, there exists a non-singular matrix $P$ such that\n",
        "$$\n",
        "A=P D P^{-1} .\n",
        "$$\n",
        "\n",
        "Let $\\mathbf{p}_1, \\ldots, \\mathbf{p}_d$ be the columns of $P$. Then\n",
        "$$\n",
        "A P=P D, \\quad\n",
        "$$\n",
        "which implies that\n",
        "$$\n",
        "A \\mathbf{p}_i=\\lambda_i \\mathbf{p}_i\n",
        "$$\n",
        "\n",
        "**Theorem 1.2.25.** If $A$ *is symmetric, then any two eigenvectors from different eigenspaces are orthogonal.*\n",
        "\n",
        "Proof. Let $\\mathbf{u}_1$ and $\\mathbf{u}_2$ be eigenvectors that correspond to distinct eigenvalues, say, $\\lambda_1$ and $\\lambda_2$. To show that $\\mathbf{u}_1 \\cdot \\mathbf{u}_2=0$, we compute\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\lambda_1 \\mathbf{u}_1 \\cdot \\mathbf{u}_2 & =\\left(\\lambda_1 \\mathbf{u}_1\\right)^T \\mathbf{u}_2=\\left(A \\mathbf{u}_1\\right)^T \\mathbf{u}_2 \\\\\n",
        "& =\\left(\\mathbf{u}_1^T A^T\\right) \\mathbf{u}_2=\\mathbf{u}_1^T\\left(A \\mathbf{u}_2\\right) \\\\\n",
        "& =\\mathbf{u}_1^T\\left(\\lambda_2 \\mathbf{u}_2\\right) \\\\\n",
        "& =\\lambda_2 \\mathbf{u}_1^T \\mathbf{u}_2=\\lambda_2 \\mathbf{u}_1 \\cdot \\mathbf{u}_2\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Since $\\mathbf{u}_1$ is an eigenvector\n",
        "Since $A^T=A$\n",
        "Since $\\mathbf{u}_2$ is an eigenvector\n",
        "\n",
        "Hence $\\left(\\lambda_1-\\lambda_2\\right) \\mathbf{u}_1 \\cdot \\mathbf{u}_2=0$. But $\\lambda_1-\\lambda_2 \\neq 0$, so $\\mathbf{u}_1 \\cdot \\mathbf{u}_2=0$.\n",
        "A matrix $A$ is said to be orthogonally diagonalizable if there are an orthogonal matrix $P$ (with $P^{-1}=P^T$ ) and a diagonal matrix $D$ such that\n",
        "$$\n",
        "A=P D P^T=P D P^{-1}\n",
        "$$\n",
        "\n",
        "To orthogonally diagonalize an $n \\times n$ matrix, we must be able to find $n$ linearly independent and orthonormal eigenvectors. If $A$ is orthogonally diagonalizable, then\n",
        "$$\n",
        "A^T=\\left(P D P^T\\right)^T=P^{T T} D^T P^T=P D P^T=A\n",
        "$$"
      ],
      "metadata": {
        "id": "aH2Zw6SARq4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus $A$ is symmetric. The following results reveal more properties of a symmetric matrix, which implies that every symmetric matrix is orthogonally diagonalizable.\n",
        "\n",
        "**Theorem 1.2.26 (The spectral theorem for symmetric matrices).** An $n \\times n$ symmetric matrix $A$ has the following properties:\n",
        "- *A has $n$ real eigenvalues, counting multiplicities.*\n",
        "- *If $\\lambda$ is an eigenvalue of $A$ with multipliaty $k$, then the eigenspace for $\\lambda$ is $k$-dimensional.*\n",
        "- *The eigenspaces are mutually orthogonal, in the sense that eigenvectors corresponding to different eigenvalues are orthogonal.*\n",
        "- *A is orthogonally diagonalizable.*\n",
        "\n",
        "*Proof.* We only give the idea of proof. If $\\mathbf{u}_1$ is a unit eigenvector corresponding to $\\lambda_1$, now start with $\\mathbf{u}_1$ and find $\\left[\\mathbf{u}_1, \\ldots, \\mathbf{u}_n\\right]$ to be an orthonormal basis by the Gram-Schmidt process and let $U=\\left[\\mathbf{u}_1, \\ldots, \\mathbf{u}_n\\right]$. Then\n",
        "$$\n",
        "U^T A U=\\left(\\begin{array}{cc}\n",
        "\\lambda_1 & * \\\\\n",
        "0 & A_1\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "\n",
        "Note that $A$ is symmetric and we must have\n",
        "$$\n",
        "U^T A U=\\left(\\begin{array}{cc}\n",
        "\\lambda_1 & 0 \\\\\n",
        "0 & A_1\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "\n",
        "Now $A_1$ must be symmetric and have the remaining eigenvalues and continuing this process, we arrive at\n",
        "$$\n",
        "U^T A U=\\left(\\begin{array}{ccc}\n",
        "\\lambda_1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "0 & \\cdots & \\lambda_n\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "\n",
        "This observation will clearly lead to the conclusions of the theorem.\n",
        "\n",
        "Suppose that $A=P D P^{-1}$, where the columns of $P$ are orthonormal eigenvectors $\\mathbf{v}_1, \\ldots, \\mathbf{v}_n$ of $A$ and the corresponding eigenvalues $\\lambda_1, \\ldots, \\lambda_n$ are in the diagonal matrix $D$. Since $P^{-1}=P^T$,"
      ],
      "metadata": {
        "id": "Dq-3bZNJSdcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "A & =P D P^T=\\left(\\begin{array}{lll}\n",
        "\\mathbf{v}_1 & \\cdots & \\mathbf{v}_n\n",
        "\\end{array}\\right)\\left(\\begin{array}{ccc}\n",
        "\\lambda_1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "0 & \\cdots & \\lambda_n\n",
        "\\end{array}\\right)\\left(\\begin{array}{c}\n",
        "\\mathbf{v}_1^T \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{v}_n^T\n",
        "\\end{array}\\right) \\\\\n",
        "& =\\left(\\begin{array}{lll}\n",
        "\\lambda_1 \\mathbf{v}_1 & \\cdots & \\lambda_n \\mathbf{v}_n\n",
        "\\end{array}\\right)\\left(\\begin{array}{c}\n",
        "\\mathbf{v}_1^T \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{v}_n^T\n",
        "\\end{array}\\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Using the column-row expansion of a product, we can write\n",
        "$$\n",
        "A=\\lambda_1 \\mathbf{v}_1 \\mathbf{v}_1^T+\\lambda_2 \\mathbf{v}_2 \\mathbf{v}_2^T+\\cdots+\\lambda_n \\mathbf{v}_n \\mathbf{v}_n^T\n",
        "$$\n",
        "\n",
        "This representation of $A$ is called a spectral decomposition of $A$ because it breaks up $A$ into pieces determined by the spectrum (eigenvalues) of $A$. Each $\\mathbf{v}_i \\mathbf{v}_i^T$ is an $n \\times n$ matrix of rank 1 . For example, every column of $\\lambda_1 \\mathbf{v}_1 \\mathbf{v}_1^T$ is a multiple of $\\mathbf{v}_1$. Furthermore, each matrix $\\mathbf{v}_j \\mathbf{v}_j^T$ is a **projection matrix** in the sense that for each $\\mathbf{x}$ in $\\mathbb{R}^n$, the vector $\\left(\\mathbf{v}_j \\mathbf{v}_j^T\\right) \\mathbf{x}$ is the orthogonal projection of $\\mathbf{x}$ onto the subspace spanned by $\\mathbf{v}_j$."
      ],
      "metadata": {
        "id": "-nUjLEoJSt57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.4.2 Constrained optimization"
      ],
      "metadata": {
        "id": "xnCMRCcWTYyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following result is useful for many optimization problems.\n",
        "**Theorem 1.2.27.** Let $A$ be $n \\times n$ symmetric matrix $A$ with an orthogonal diagonalization $A=P D P^{-1}$. The columns of $P$ are orthonormal eigenvectors $\\mathbf{v}_1, \\ldots, \\mathbf{v}_n$ of $A$. Assume that the diagonals of $D$ are arranged so that $\\lambda_1 \\leq \\lambda_2, \\ldots \\leq \\lambda_n$. Then\n",
        "$$\n",
        "\\min _{\\mathbf{x} \\neq 0} \\frac{\\mathbf{x}^T A \\mathbf{x}}{\\mathbf{x}^T \\mathbf{x}}=\\lambda_1\n",
        "$$\n",
        "is achieved when $\\mathbf{x}=\\mathbf{v}_1$ and\n",
        "$$\n",
        "\\max _{\\mathbf{x} \\neq 0} \\frac{\\mathbf{x}^T A \\mathbf{x}}{\\mathbf{x}^T \\mathbf{x}}=\\lambda_n\n",
        "$$\n",
        "is achieved when $\\mathbf{x}=\\mathbf{v}_{\\mathbf{n}}$.\n",
        "*Proof.* From the assumption, we have\n",
        "$$\n",
        "A=P\\left(\\begin{array}{lll}\n",
        "\\lambda_1 & & \\\\\n",
        "& \\ddots & \\\\\n",
        "& & \\lambda_n\n",
        "\\end{array}\\right) P^T\n",
        "$$"
      ],
      "metadata": {
        "id": "O3BvZ4h2TeW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "and\n",
        "$$\n",
        "P=\\left[\\begin{array}{lll}\n",
        "\\mathbf{v}_1 & \\cdots & \\mathbf{v}_{\\mathrm{n}}\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "Rearranging the terms gives\n",
        "$$\n",
        "P^T A P=\\left(\\begin{array}{ccc}\n",
        "\\lambda_1 & & \\\\\n",
        "& \\ddots & \\\\\n",
        "& & \\lambda_n\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "\n",
        "In addition, note that\n",
        "$$\n",
        "\\begin{aligned}\n",
        "A \\mathbf{v}_{\\mathbf{i}} & =\\lambda_i \\mathbf{v}_{\\mathbf{i}} \\\\\n",
        "\\mathbf{x} & =P \\mathbf{y}\n",
        "\\end{aligned}\n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\sum x_i^2=\\sum y_i^2\n",
        "$$\n",
        "\n",
        "It is easy to see that\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\mathbf{x}^T A \\mathbf{x}}{\\sum x_i^2}=\\frac{\\mathbf{y}^T P^T A P \\mathbf{y}}{\\sum y_i^2} & =\\frac{\\lambda_1 y_1^2+\\cdots+\\lambda_n y_n^2}{\\sum y_i^2} \\\\\n",
        "& \\geq \\lambda_1\\left(\\text { equality holds when } \\mathbf{y}=\\left(\\begin{array}{c}\n",
        "1 \\\\\n",
        "0 \\\\\n",
        "\\vdots \\\\\n",
        "0\n",
        "\\end{array}\\right)\\right. \\text { ) } \\\\\n",
        "& \\leq \\lambda_n\\left(\\text { equality holds when } \\mathbf{y}=\\left(\\begin{array}{c}\n",
        "0 \\\\\n",
        "\\vdots \\\\\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right)\\right.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Note that\n",
        "$$\n",
        "\\mathbf{v}_{\\mathbf{1}}=P\\left(\\begin{array}{c}\n",
        "1 \\\\\n",
        "0 \\\\\n",
        "\\vdots \\\\\n",
        "0\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "\n",
        "and\n",
        "$$\n",
        "\\mathbf{v}_{\\mathbf{n}}=P\\left(\\begin{array}{c}\n",
        "0 \\\\\n",
        "0 \\\\\n",
        "\\vdots \\\\\n",
        "1\n",
        "\\end{array}\\right)\n",
        ".$$"
      ],
      "metadata": {
        "id": "-6yGDvUvTe0U"
      }
    }
  ]
}